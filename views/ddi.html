<html>
	<head>
		<!--Import Google Icon Font-->
        <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel='stylesheet'>
		<!-- Links to compiled and minified CSS on CDN server -->
		<link rel='stylesheet' href='/static/css/materialize.min.css'>
		<!-- Links to compiled and minified materialize javascript file on CDN Server -->
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
		<script src='/static/js/materialize.min.js' ></script>
		<link rel='stylesheet' href='/static/css/style.css'>
	</head>
	<title>Vikram M. Reddy</title>
	<body>
	<div class='container'>
	<h2 class='center pink-text text-darken-2'>
		Predicting Drug-Drug Interactions <br>
	</h2>
	<h5 class='center pink-text text-darken-1'>
		Authors: Vikram Reddy, Bhuvana Bellala, Sameer Bajaj, and Nicolas Mon
	</h5>
	<h5 class='center pink-text text-darken-1'>
		Final Project for NLP course at UC Berkeley, taught by Prof. Marti Hearst, Fall 2016
	</h5>
		<div class='flow-text'><br>
			
<b>Introduction:</b><br>
Drug to drug interactions are a subcategory of adverse drug reactions (ADRs). According to a study in 2013, ADRs account for 300,000 deaths annually across Europe and the United States (Businaro, 2013). The latest biomedical research papers describe new drugs that come on the market and their potential adverse effects. The automatic extraction and classification of these potential drug to drug interactions would assist a doctor in deciding whether or not to prescribe a certain new drug to a patient.<br><br>
Our project proposed to classify drug pairs as exhibiting an interaction or not. We used the dataset from the 2013 DDI Extraction Challenge. It consisted of 792 biomedical texts from DrugBank. These texts were parsed into sentences and then run through MetaMap to extract potential drug pairs. MetaMap was used to identify sentences that contain pharmacological substances as detected by Metathesaurus. Each sentence was then manually annotated by experts to create a dataset of statements that unambiguously identify 1) interacting drug pairs, and 2) non-interacting drug pairs[1] (Aronson). The Inter-Annotator Agreement for the DrugBank documents were all above a Kappa coefficient of 0.80, with some reaching a Kappa of 0.96. For the Medline abstracts, the Inter-Annotator Agreement was between 0.55 to 0.72 (Herrero-Zazo et. al, 2013).<br><br>
Our goal was to featurize each sentence in the parsed biomedical data, associate that sentence or fragment to a drug pair, and train a classifier to predict future drug pairs. The state-of-the-art F1 score during the challenge was 67, and we had an F1 score of 59 on our best result. Our best model used lexical, syntactic, and semantic features and ran a cross-validated random forest as the classifier.<br><br>
<b>Data Challenges:</b><br>
There were several challenges in the data that we addressed during preprocessing and featurization. First, in a single sentence there might be more than 2 drug names. If so, each sentence would have 3 or more drug pairs associated with it. To differentiate the data associated with each drug pair, we split up the sentence into 3 fragments. These fragments--before drug 1, between drug 1 and drug 2, and after drug2--were treated as
 separate bag of words for each drug pair. In this way, the feature vectorization for each drug pair would be slightly different, which might mitigate the effects of drug name repetition.
Secondly, a preliminary analysis showed the dataset is highly unbalanced towards non-interacting drug pairs. We took two measures to account for this skew. Based on the suggestions from Bobic, Fluck and Hofmann-Apitius, we performed random undersampling of the data and obtained a balanced dataset with about 50% interacting pairs and 50% non-interacting pairs[11]. Second, we primarily used the F1 score to judge our results. Since F1 score is a measure of both precision and recall, we were able to drive the feature set development based on how well our classifier is performing on the interacting drug pairs.<br><br>
Thirdly, if the name of a drug, say ‘drug1’, appeared more than once in a sentence, the corpus included the pair <’drug1’, ‘drug1’> as a potential interacting pair. However, after reading Bui’s paper, we excluded such pairs to avoid confusing our model[2].
Finally, we augmented the dataset by adding three key columns. The first two columns are called ‘drug1offset’ and ‘drug2offset’. These two columns represent the character offsets of the two drugs in a given sentence. For example, in the sentence ‘Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance’, MTX is at 50-52 and TNF blocking agents is from 84-102. These two columns helped us segment the sentences correctly during feature extraction because some drugs were spread across the sentence or appeared more than once within the same sentence. The third column is called ‘drugsinsent’. This column contained all the drugs that were mentioned in a given sentence. Using this column, we were able to quickly build features that took into account the relation between all the drugs mentioned in a sentence.<br><br>
<b>Two-Stage Classification:</b><br>
Another way to address the unbalanced data was to apply a 2-stage classification model. This is used if we were to classify the interacting pairs further into 4 subcategories--int, advice, mechanism, and effect. One of the teams in the 2013 DDI Extraction Challenge, UWM-Triads, used this in combination with preprocessing to deal with the unbalanced data. About four-fifths of the data exhibited non-interactions and within the interactions, only about 1/15 were characterized as “int”. Thus, the training set was heavily skewed.
 As researcher Majid Rastegar-Mojarad explains, data should be preprocessed to prepare for the first stage of binary classification, then post-processed to prepare for the second stage of multivariable classification (Rastegar-Mojarad). We applied this methodology to the baseline model using only TFIDF features. It was beyond the scope of our project to classify interactions into 4 categories, so we left the 2-stage classification out of the final pipeline. We have included it in the code as a separate file. We will use this method for future classification work with unbalanced data.<br><br>
<b>Feature Extraction/Description of Algorithms:</b><br>
We used many feature extraction methods that we read about in various research papers. We read about 15 papers from the various participants in the data challenge, as well as from other DDI Extraction researchers. We have listed these papers in the Works Cited section.
Our features used a bag of words model, regex parsing, and dependency parsing. For all the features in the bag of words models, we first segmented the sentence into three parts. The first segment contains the words before the mention of the first drug, the second segment contains the words in between the two drug mentions, and the last segment contains the words after the second drug mention. After segmenting the sentence into three parts, we built the following features:<br><br>
<b>Lexical Features:</b><br>
In order to capture the token information around each pair, for each sentence fragment, we counted the number of words, the number of drug mentions, the number of verbs in a parse tree, the number of trigger words, and the number of negation words. We researched a list of trigger words, such as “administer”, “concomitantly”, “increase”, “decrease”, and “absorption”, and inferred that these words would be good indicators of an interaction (Bui). We also researched a list of negation words, such as “not”, “neither”, and “unable” (Chowdhury 2011) to account for negation of a trigger, such as “does not increase” or “unchanged”.<br><br>
In addition, we also implemented a feature that returned the distance of the closest trigger word to a drug mention. The idea behind this feature was to indicate sentences in which a trigger word appeared very close to a drug mention, as trigger words closer to drug mentions could mean a higher chance that that sentence indicates a reaction. Another reason to calculate the distance from trigger to negation and from negation to the trigger
 is to incorporate relationships between words in the sentences. Some sentences have repetitions of the same words (as described in the Data Challenges section) that appear in different parts of the sentences. One method to address this would be to simply delete the repetitions during normalization, but we decided that this would be a loss of information. Instead we tried to encode this distance in the features. Granted, the distance count is a loose feature for this end goal. After this idea, we were motivated to implement dependency parser and word2vec features to get a better representation of the grammatical and semantic relationships within the text.<br><br>
<b>Dependency parser features:</b><br>
Many of the sentences in our dataset have a very complex structure, and some words were related in ways that were not apparent in simply the location of the word in the sentence. In order to first understand the syntactic nature of our sentences, we built a parser that chunked our sentences, into noun-phrases, verb-phrases, prepositional-phrases, phrases (contained one or more noun- and prepositional-phrases), and clauses (a phrase followed by a verb phrase followed by a noun phrase). The parser was modeled after the suggestion made by Bui and et al. Here is a sample of the tree built by our parser[2]:<br><br>
 After analyzing the syntactic structure of several sentences, we decided to use a dependency parser to capture these relationships. Using the spacy dependency parser, we built the following features:<br>
● One  feature attempted to capture the ‘subject-direct object’ relationship that would be present in sentences like ‘Drug1 effects Drug2’. In that sentence, Drug1 is the subject and Drug2 is the direct object. Oftentimes, however, this relationship would not be as straightforward. For example, you could have the sentence ‘Drug1 increases the absorption of Drug2’. In this case, absorption is the direct object and Drug2 is in a subordinate prepositional phrase. To address this, the
 ‘subject-object’ feature searched the entire subtrees of the subject and direct object of the sentence, returning 1 only if one drug was the subject or in its subtree, and the other was the direct object or in its subtree.<br>
● Another  feature implemented using the dependency parser searched for a trigger word in the dependency path from each drug mention to the root verb of the sentence. A third binary feature using the Spacy dependency parser would check if both drug mentions had the same head, meaning they both depended on the same word.<br><br>
<b>Sentiment Analysis:</b><br>
Based on the intuition that interactions between two drugs could either be positive or negative, we investigated the effect of sentiment analysis features in locating drug interaction mentions in sentences. We addressed the task of locating drug interactions mentions in the sentence text using heuristics to measure the strength of positive or negative sentiment in the sentence text, so that we can classify the remaining neutral sentences as non-interactions. Given the input text, we defined an algorithm that would calculate the overall sentiment value to provide a positive, negative, neutral and compound score to the sentence. With sentiment analysis, we saw marginal improvements in accuracy and F1 score. It was not surprising to us to see a small change in accuracy due to the complex sentence structure of the dataset. For example, the sentence text - 'Although MIVACRON (a mixture of three stereoisomers) has been administered safely following succinylcholine-facilitated tracheal intubation, the interaction between MIVACRON and succinylcholine has not been systematically studied,' scored a high negative score and a mid-range neutral score on our sentiment analysis algorithm, however, the drugs mentioned in the sentence do not interact with one another. With the results of the F1 Score, we realized that sentiment analysis would only have a marginal effect on improving the accuracy of our model.<br><br>
<b>Word2Vec Features:</b><br>
Another helpful feature to learn the semantic meaning of the words is to use word embeddings. The word2vec module of gensim groups contextually similar words together. The vectorization and dimensionality reduction of these words would help to extract the concepts from the text. The bag of words approach, by contrast, is prone to problems of homophony and polysemy (Crain). To take advantage of the semantic meaning around each drug entity, we ran the gensim model on the sentences in out train
 dataset. Then for each sentence within our dataset we tokenized and tagged the sentences. In order to capture the immediate context of each drug and the relationship between the two, we traversed through the sentence and found the closest verb or noun and vectorized the token using our word2vec model. Once vectorized, we took the mean of the model. Our intention with this feature was to build a classifier that understood the context of a given entity pair. With the addition of this feature, we saw a marked increase in out F1 score from 58 to 59.1.<br><br>
<b>Classifiers and GridSearchCV:</b><br>
Initially, we used a MultionomialNB with alpha=1.15 as our classifier. However, even with the addition of multiple features, we never saw an improvement in out F1 Score from 42. So, we tried Logistic regression, SVM, AdaBoostClassifer, KNeighborsClassifier, and RandomForestClassifer. For each classifier, we ran a GridSearchCV in order to fine tune the parameters. In the end, our best classifier was the RandomForestClassifier with an accuracy of 80.53% and an F1 score of 59.75 for the interacting drug-drug pairs. For the RandomForestClassifer, the GridSearchCV optimized the n_estimators and max_depth parameters.<br><br>
<b>Additional Feature Selection:</b><br>
One way to computationally reduce the size of our feature set was to use the Variance Threshold functionality. We set a minimum for the within-feature variance when running without the dependency parser and word2vec features. This improved the accuracy from .28 to .39, so we reasoned that many of the simple bag of words features did not contribute as much to the score.<br><br>
<b>Error Analysis:</b><br>
After extracting several features and generating the predicted output, we created an excel spreadsheet for error analysis. Each row had the sentences, the drug pairs, the expected value, the predicted value, and the list of contributing features. From this document, we were able to pinpoint for which sentences our algorithm made mistakes. We also found new challenges in the data. For example, one sentence had an ambiguous description--that one drug would increase, decrease, or have no effect on the second drug. This drug-drug pair was manually annotated as having an interaction, despite its ambiguity. We realized that drugs having a small fractional chance of an interaction, should be classified as a 1, so our algorithm had to function accordingly.
 Also, to pinpoint what terms resulted in interactions and non-interactions, we visualized the data on the training set. We first chose a random sample of 300-600 terms and then used multidimensional scaling to map the features to a 2 dimensional space. Then we used matplotlib with d3 tooltips to identify which key terms resulted in positive interactions. The code for this was largely used from Brandon Rose’s clustering notebook (Rose). These key terms were used to infer possible feature extraction. There were several interactions with types of enzymes labeled. For example, the bigram “are cyp3a4” is a reference to an enzyme which is involved in digestion. We reasoned that it could be classified as an interaction because the biomedical research articles that contain this term would be discussing the drug’s interaction with this particular enzyme. Thus, for future work, we could collate a list of known enzymes in drug to drug interactions, which we could append to our list of key “trigger” words.<br><br>
<b>Results:</b><br>
Our baseline accuracy established by running a MultinomialNB on a vectorized bag of words was 42.965% on the pseudo-balanced dataset. With the addition of features and model optimization, we were able to increase the accuracy up to 80.532%.<br><br>
Also, our baseline F1 score for interacting drug pairs was 0.36. We have come a long way by customizing our features and optimizing our models to reach an F1 score of ~0.597. A number of useful features such as locating trigger words, drug names and running dependency parsers helped increase the F1 score of our model. Apart from experimenting with numerous features, we also tried our best to find the most optimal model for our dataset. By tuning parameters, and running a grid search, we were able to achieve a considerably high F1 score of 0.6.<br><br>
<b>Future work:</b><br>
Since the 2013 DDI Challenge, there has been new research using the same dataset. For example, Zhao uses convolutional neural networks to predict interactions and achieves an F1 score of 68.8, which beat the winner of the challenge (Zhao et al., 2016). Zhao’s CNNs were based on syntactic word embeddings. If we were to continue our project, we would use our word2vec features as inputs to a nonlinear neural network. In the current state of our project, we used word2vec features as inputs to a linear model. We would also try using a semantic parser to extract the relationships between the words rather than just having collocations or keywords as features in isolation.<br><br>
In addition, another way we could expand on our project would be to change our classifier from a binary classifier (predicting interactions or non-interactions) to one that predicted what type of interaction that the sentence signified. This would use a 2-stage classification as described earlier.
	</div> <!--end text-->
	</div> <!--end container-->
	        <footer id='contact' class='page-footer pink lighten-2'>
        	<div class='container'>
        		<div class='row'>
        			<div class='col s6 m6 l6'>
                	<h5 class='white-text'>Contact</h5>
                	<p class="grey-text text-lighten-4">Feel free to reach out to me on my professional networks. I'm always happy to chat about new ideas. </p>
             	</div>
              <div class='col l4 offset-l2 s4 offset-s2'>
                <h5 class='white-text'>Links</h5>
                <ul>
                  <li><a class='grey-text text-lighten-4' href='http://github.com/ghvik'>github</a></li>
                  <li><a class='grey-text text-lighten-4' href='https://www.linkedin.com/in/vikram-reddy'>linkedin</a></li>
                  <li><a class='grey-text text-lighten-4' href='https://www.ischool.berkeley.edu/people/vikram-reddy'>student profile @berkeley</a></li>
                </ul>
              </div>
            </div>
          </div>
          <div class="footer-copyright">
            <div class="container">
            © 2017 Vikram M. Reddy
            <a class="grey-text text-lighten-4 right" href="http://vikreddy.herokuapp.com">homepage</a>
            </div>
          </div>
        </footer>
	</body>
</html>